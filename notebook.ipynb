{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import executable, argv\n",
    "from subprocess import check_output\n",
    "import pctils as pc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import executable, argv\n",
    "from subprocess import check_output\n",
    "import pctils as pc\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import spacy\n",
    "import hashlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pedrocondeco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformers\n",
    "def apply_tokenizer(data, tokenizer):\n",
    "    lista_tok = [\" \".join(tokenizer.tokenize(x)) for x in data]\n",
    "    return lista_tok\n",
    "\n",
    "def apply_lowercase(data):\n",
    "    list_lower = [x.lower() for x in data]\n",
    "    return list_lower\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "def apply_filter_stopwords(data, stopword_list):\n",
    "    data_no_stopwords = []\n",
    "    for x in data:\n",
    "        aux = \" \".join([w for w in x.split() if w not in stopword_list])\n",
    "        data_no_stopwords.append(aux)\n",
    "    return data_no_stopwords\n",
    "\n",
    "def apply_filter_punct(data):\n",
    "    data_no_punct = []\n",
    "    for x in data:\n",
    "        \n",
    "        aux = \"\".join([w for w in x if w not in string.punctuation])\n",
    "        data_no_punct.append(aux)\n",
    "\n",
    "    return data_no_punct\n",
    "\n",
    "def normalize_whitespace(data):\n",
    "    return [re.sub(r\"^\\s+|\\s+$|(?<=\\s)\\s*\", \"\", text) for text in data]\n",
    "\n",
    "def apply_stemmer(data, stemmer):\n",
    "    list_tok = [WordPunctTokenizer().tokenize(x) for x in data]\n",
    "    stems = [\" \".join(list(map(stemmer.stem, y))) for y in list_tok]\n",
    "    return stems\n",
    "\n",
    "# creates a new column helpful_count/rates_count\n",
    "class TransformLabels(TransformerMixin):\n",
    "    def __init__(self, threshold = .5):\n",
    "        self.helpful_count = 'helpful_count'\n",
    "        self.rates_count = 'rates_count'\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def isHelpful(self, helpful_count, rates_count):\n",
    "        if rates_count == 0:\n",
    "            return 0 # we don't know, not helpfull for now\n",
    "        if helpful_count > rates_count:\n",
    "            return 5 # five means bad, needs to be droped\n",
    "        val = helpful_count/rates_count\n",
    "        if val >= self.threshold:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "\n",
    "        label_array = []\n",
    "        for h, r in zip(_X[self.helpful_count], _X[self.rates_count]):\n",
    "            is_helpul = self.isHelpful(h,r)\n",
    "            label_array += [is_helpul]\n",
    "\n",
    "        out = pd.Series(label_array)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class TextTransformer(TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        words = _X[self.key]\n",
    "        features =  TfidfTransformer().fit_transform(words)\n",
    "        return features\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class LazyTranformer(TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        return _X[self.key].to_frame()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "class NumberSelector(Selector):\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, column_key, tokenizer, lower=True, remove_punct=True, stopwords=[], stemmer=None):\n",
    "        self.column_key = column_key\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, data):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        sentences_preprocessed = apply_tokenizer(data, self.tokenizer)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "            sentences_preprocessed = apply_lowercase(sentences_preprocessed)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        if self.stopwords:\n",
    "            sentences_preprocessed = apply_filter_stopwords(sentences_preprocessed,self.stopwords)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            sentences_preprocessed = apply_filter_punct(sentences_preprocessed)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        sentences_preprocessed = normalize_whitespace(sentences_preprocessed)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "    \n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            sentences_preprocessed = apply_stemmer(sentences_preprocessed,self.stemmer)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        return sentences_preprocessed\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        column_to_transform = _X[self.column_key]\n",
    "        return self.clean_sentences(column_to_transform)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "#data_set = pc.getFile()  # gets path of tfile, you may replace with a string\n",
    "data_set=\"/home/pedrocondeco/ldsa/group3/data/book_review_labelled_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "#data_set_to_generalize = pc.getFile()  # gets path of tfile, you may replace with a string\n",
    "data_set_to_generalize=\"/home/pedrocondeco/ldsa/group3/data/book_review_test_data_unlabelled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "label = \"label\" # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(data_set) \n",
    "df['reviewText'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"reviewText\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df)\n",
    "df['summary'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"summary\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df)\n",
    "\n",
    "# Load the stuff to generalize\n",
    "df_to_generalize = pd.read_csv(data_set_to_generalize) \n",
    "df_to_generalize['reviewText'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"reviewText\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df_to_generalize)\n",
    "df_to_generalize['summary'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"summary\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df_to_generalize)\n",
    "\n",
    "\n",
    "\n",
    "# Show basic info\n",
    "print(df.dtypes)            # all columns/types\n",
    "print(\"Shape: \",df.shape)   # shape\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries:  49992\n",
      "\n",
      "Distribution of the column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    27871\n",
       "1    22121\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nan Values:  0\n"
     ]
    }
   ],
   "source": [
    "# Add label to df (without this it's not possible to train the model)\n",
    "df[label] = TransformLabels(threshold=.5).fit_transform(df)  \n",
    "# Show label distribution\n",
    "pc.showColumnDistribution(df,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop bad stuff?\n",
    "# df = df[[df != 5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedrocondeco/.local/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_label = df_train['label']\n",
    "df_train.drop(label, inplace=True, axis=1)   # labels MUST NOT exist within train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features here, keep standard, concatenate _feature in the end\n",
    "review_text_feature = Pipeline([\n",
    "                                ('review_text_selector', TextSelector(\"reviewText\")),\n",
    "                                ('tfidf', TfidfVectorizer())\n",
    "                            ])\n",
    "summary_feature = Pipeline([\n",
    "                                ('summary_selector', TextSelector(\"summary\")),\n",
    "                                ('tfidf', TfidfVectorizer())\n",
    "                            ])\n",
    "rating_feature = Pipeline([\n",
    "                                ('rating_selector', NumberSelector(\"rating\")),\n",
    "                                ('onehot', OneHotEncoder())\n",
    "                            ])\n",
    "# Features\n",
    "features = FeatureUnion([\n",
    "                        ('summary_feature',summary_feature),\n",
    "                        ('review_text_feature',review_text_feature)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline w/ model\n",
    "model_pipeline = Pipeline([\n",
    "                        ('features', features),\n",
    "                        ('model',RandomForestClassifier(random_state = 42))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline and print silly values\n",
    "# returns model\n",
    "def baseline(train_set, train_labels, test_set, model):\n",
    "  # Train the classifier\n",
    "  model.fit(train_set, train_labels)\n",
    "\n",
    "  # Predict\n",
    "  model_predictions = model.predict(test_set)\n",
    "\n",
    "  # Print F1 stuff\n",
    "  print(classification_report(test_set[label], model_predictions))\n",
    "\n",
    "  _acc = np.mean(model_predictions == test_set[label])\n",
    "  print('Accuracy: ' + str(_acc))\n",
    "\n",
    "  return model\n",
    "\n",
    "def stupidFunction(x):\n",
    "    if x == 0:\n",
    "        return \"false\"\n",
    "    return \"true\"\n",
    "def predictFromFileAndOutPutCSV(model, path):\n",
    "    data_set_to_predict = pc.getFile()\n",
    "    df = pd.read_csv(data_set_to_predict) # never modify\n",
    "    predictions = model.predict(df)\n",
    "    out = pd.DataFrame({'is_helpful': predictions})\n",
    "    out['is_helpful'] =  out['is_helpful'].apply(lambda x: stupidFunction(x))\n",
    "    out.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74      5590\n",
      "           1       0.68      0.54      0.60      4409\n",
      "\n",
      "    accuracy                           0.68      9999\n",
      "   macro avg       0.68      0.67      0.67      9999\n",
      "weighted avg       0.68      0.68      0.68      9999\n",
      "\n",
      "Accuracy: 0.6845684568456846\n"
     ]
    }
   ],
   "source": [
    "model = baseline(df_train, train_label, df_test, model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4124/1423091582.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictFromFileAndOutPutCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'done.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4124/17325560.py\u001b[0m in \u001b[0;36mpredictFromFileAndOutPutCSV\u001b[0;34m(model, path)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredictFromFileAndOutPutCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdata_set_to_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_set_to_predict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# never modify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ldsa/group3/pctils.py\u001b[0m in \u001b[0;36mgetFile\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcustomfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui_fname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshowColumnDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ldsa/group3/open_file.py\u001b[0m in \u001b[0;36mgui_fname\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m      7\u001b[0m     the chosen filename\"\"\"\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# run this exact file in a separate process, and grab the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexecutable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0;32m--> 411\u001b[0;31m                **kwargs).stdout\n\u001b[0m\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m    949\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictFromFileAndOutPutCSV(model, 'done.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}