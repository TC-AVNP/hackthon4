{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import executable, argv\n",
    "from subprocess import check_output\n",
    "import pctils as pc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import executable, argv\n",
    "from subprocess import check_output\n",
    "import pctils as pc\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import spacy\n",
    "import hashlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pedrocondeco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformers\n",
    "def apply_tokenizer(data, tokenizer):\n",
    "    lista_tok = [\" \".join(tokenizer.tokenize(x)) for x in data]\n",
    "    return lista_tok\n",
    "\n",
    "def apply_lowercase(data):\n",
    "    list_lower = [x.lower() for x in data]\n",
    "    return list_lower\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "def apply_filter_stopwords(data, stopword_list):\n",
    "    data_no_stopwords = []\n",
    "    for x in data:\n",
    "        aux = \" \".join([w for w in x.split() if w not in stopword_list])\n",
    "        data_no_stopwords.append(aux)\n",
    "    return data_no_stopwords\n",
    "\n",
    "def apply_filter_punct(data):\n",
    "    data_no_punct = []\n",
    "    for x in data:\n",
    "        \n",
    "        aux = \"\".join([w for w in x if w not in string.punctuation])\n",
    "        data_no_punct.append(aux)\n",
    "\n",
    "    return data_no_punct\n",
    "\n",
    "def normalize_whitespace(data):\n",
    "    return [re.sub(r\"^\\s+|\\s+$|(?<=\\s)\\s*\", \"\", text) for text in data]\n",
    "\n",
    "def apply_stemmer(data, stemmer):\n",
    "    list_tok = [WordPunctTokenizer().tokenize(x) for x in data]\n",
    "    stems = [\" \".join(list(map(stemmer.stem, y))) for y in list_tok]\n",
    "    return stems\n",
    "\n",
    "# creates a new column helpful_count/rates_count\n",
    "class TransformLabels(TransformerMixin):\n",
    "    def __init__(self, threshold = .5):\n",
    "        self.helpful_count = 'helpful_count'\n",
    "        self.rates_count = 'rates_count'\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def isHelpful(self, helpful_count, rates_count):\n",
    "        if rates_count == 0 or rates_count < 5:\n",
    "            return 5 # we don't know, not helpfull for now\n",
    "        if helpful_count > rates_count:\n",
    "            return 5 # five means bad, needs to be droped\n",
    "        val = helpful_count/rates_count\n",
    "        if val >= self.threshold:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "\n",
    "        label_array = []\n",
    "        for h, r in zip(_X[self.helpful_count], _X[self.rates_count]):\n",
    "            is_helpul = self.isHelpful(h,r)\n",
    "            label_array += [is_helpul]\n",
    "\n",
    "        out = pd.Series(label_array)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class TextTransformer(TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        words = _X[self.key]\n",
    "        features =  TfidfTransformer().fit_transform(words)\n",
    "        return features\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class LazyTranformer(TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        return _X[self.key].to_frame()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "class NumberSelector(Selector):\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, column_key, tokenizer, lower=True, remove_punct=True, stopwords=[], stemmer=None):\n",
    "        self.column_key = column_key\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, data):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        sentences_preprocessed = apply_tokenizer(data, self.tokenizer)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "            sentences_preprocessed = apply_lowercase(sentences_preprocessed)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        if self.stopwords:\n",
    "            sentences_preprocessed = apply_filter_stopwords(sentences_preprocessed,self.stopwords)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            sentences_preprocessed = apply_filter_punct(sentences_preprocessed)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        sentences_preprocessed = normalize_whitespace(sentences_preprocessed)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "    \n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            sentences_preprocessed = apply_stemmer(sentences_preprocessed,self.stemmer)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        return sentences_preprocessed\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        column_to_transform = _X[self.column_key]\n",
    "        return self.clean_sentences(column_to_transform)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word size \n",
    "def get_av_word_size(docs):\n",
    "    av_words_size_per_doc = [] \n",
    "    for doc in docs: \n",
    "        words_size = [len(word) for word in doc.split()]\n",
    "        av_size = np.mean(np.array(words_size))\n",
    "        av_words_size_per_doc.append(av_size)\n",
    "    return av_words_size_per_doc\n",
    "# get_av_word_size(docs)  \n",
    "\n",
    "# number of words \n",
    "def get_nb_words(docs): \n",
    "    return ([len(doc.split()) for doc in docs])\n",
    "\n",
    "# get_nb_words(docs)\n",
    "\n",
    "# number of unique words (proxy for how rich is the reviewer vocabulary)\n",
    "# not really sure if this feature will be useful (maybe couple with PoS) \n",
    "def get_nb_unique_words(docs):\n",
    "    unique_words = [len(set(word for word in doc)) for doc in docs] \n",
    "    normalized = (np.array(unique_words)/np.array(get_nb_words(docs))).tolist()\n",
    "    return normalized\n",
    "# get_nb_unique_words(docs)\n",
    "\n",
    "# getting NERs (proxy for attention to detail by reviewers that describe book parts, e.g. naming book charters)\n",
    "def get_most_common_persons(nlp_docs, n_most_common=10): \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [[{'ENT_TYPE':'PERSON'}]]\n",
    "    matcher.add('ppl', pattern)\n",
    "\n",
    "    persons = []\n",
    "    for doc in nlp_docs: \n",
    "        matches = matcher(doc)\n",
    "        for match_id, start, end in matches: \n",
    "            persons.append(str(doc[start:end]))\n",
    "\n",
    "    most_common_ents = [person for person in Counter(persons).most_common()[:n_most_common]]\n",
    "    return most_common_ents\n",
    "\n",
    "# get_most_common_persons(nlp_docs)    \n",
    "\n",
    "# number of predefined parts of speech \n",
    "def get_nb_PoS(nlp_docs): # docs tokenized string  \n",
    "    PoS_list = ['ADJ','NOUN','VERB'] \n",
    "    nb_PoS = [] \n",
    "    for doc in nlp_docs: \n",
    "        total_PoS = [token.pos_ for token in doc if token.pos_ in PoS_list]  \n",
    "        nb_in_doc = sum([Counter(total_PoS)[x] for x in PoS_list])\n",
    "        nb_PoS.append(nb_in_doc)\n",
    "    return nb_PoS\n",
    "\n",
    "# get_nb_PoS(nlp_docs)\n",
    "\n",
    "def get_nb_unique_PoS(nlp_docs): # docs tokenized string  \n",
    "    PoS_list = ['ADJ','NOUN'] \n",
    "    nb_unique_PoS = [] \n",
    "    nb_PoS = []\n",
    "    word4PoS = [] \n",
    "    for doc in nlp_docs: \n",
    "        total_PoS = [token.pos_ for token in doc]\n",
    "        mask = np.array([True if PoS in PoS_list else False for PoS in total_PoS])\n",
    "        # print(mask[:10])\n",
    "        words = np.array([token.text for token in doc])\n",
    "        words_under_selected_PoS = words[mask].tolist()\n",
    "        word4PoS.append(words_under_selected_PoS)\n",
    "        # print(words_under_selected_PoS)\n",
    "        unique_words = get_nb_unique_words(words_under_selected_PoS)\n",
    "        nb_unique_PoS.append(len(unique_words))\n",
    "        nb_PoS.append(len(words_under_selected_PoS))\n",
    "        \n",
    "    ratio = np.array(nb_unique_PoS)/ np.array(nb_PoS)\n",
    "    return ratio.tolist()\n",
    "    \n",
    "# get_nb_unique_PoS(nlp_docs)\n",
    "\n",
    "def add_extra_features(df, someString,  docs, nlp_docs): \n",
    "    df[someString+\"nb_words\"] = pd.Series(get_nb_words(docs)).apply(lambda x: 0 if x == None or x == np.NaN  else x)\n",
    "    df[someString+\"av_word_size\"] = pd.Series(get_av_word_size(docs)).apply(lambda x: 0 if x == None or x == np.NaN  else x)\n",
    "    df[someString+\"nb_unique_words\"] = pd.Series(get_nb_unique_words(docs)).apply(lambda x: 0 if x == None or x == np.NaN  else x)\n",
    "    \n",
    "    \n",
    "    # get_most_common_persons(nlp_docs, n_most_common=10) \n",
    "    \n",
    "    df[someString+\"nb_PoS\"] = pd.Series(get_nb_PoS(nlp_docs)).apply(lambda x: 0 if x == None or x == np.NaN  else x)\n",
    "    #df[\"nb_unique_PoS\"] = pd.Series(get_nb_unique_PoS(nlp_docs))\n",
    "\n",
    "    #df[someString+\"nb_words\"] = df[someString+\"nb_words\"].fillna(0, inplace=True)\n",
    "    #df[someString+\"av_word_size\"] = df[someString+\"av_word_size\"].fillna(0, inplace=True)\n",
    "    #df[someString+\"nb_unique_words\"] = df[someString+\"nb_unique_words\"].fillna(0, inplace=True)\n",
    "    #df[someString+\"nb_PoS\"] = df[someString+\"nb_PoS\"].fillna(0, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "#data_set = pc.getFile()  # gets path of tfile, you may replace with a string\n",
    "data_set=\"/home/pedrocondeco/ldsa/group3/data/book_review_labelled_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "#data_set_to_generalize = pc.getFile()  # gets path of tfile, you may replace with a string\n",
    "data_set_to_generalize=\"/home/pedrocondeco/ldsa/group3/data/book_review_test_data_unlabelled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "label = \"label\" # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewerID       object\n",
      "reviewerName     object\n",
      "reviewText       object\n",
      "overall           int64\n",
      "summary          object\n",
      "reviewTime       object\n",
      "rates_count       int64\n",
      "helpful_count     int64\n",
      "rating            int64\n",
      "dtype: object\n",
      "Shape:  (49992, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>rates_count</th>\n",
       "      <th>helpful_count</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3UPFTGAWZ3G2R</td>\n",
       "      <td>David J. Loftus</td>\n",
       "      <td>jenkin histori professor member parliament wel...</td>\n",
       "      <td>4</td>\n",
       "      <td>quit readabl nice done</td>\n",
       "      <td>12 6, 2001</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1XTKTLNSCRLDS</td>\n",
       "      <td>Ellen Rappaport</td>\n",
       "      <td>detect inspector erlendur sveinsson best uncov...</td>\n",
       "      <td>5</td>\n",
       "      <td>mesmer depth</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1A77B6DQQH436</td>\n",
       "      <td>crescamp \"esc\"</td>\n",
       "      <td>read purchas gift famili small children hope m...</td>\n",
       "      <td>3</td>\n",
       "      <td>10 minut life lesson kid</td>\n",
       "      <td>02 12, 2013</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEAF4MRYHJZI</td>\n",
       "      <td>Angelia Menchan \"acvermen.blogspot.com\"</td>\n",
       "      <td>fierc angel sheri park read like dissert afric...</td>\n",
       "      <td>4</td>\n",
       "      <td>fierc</td>\n",
       "      <td>03 24, 2010</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3B7KU72LGWFER</td>\n",
       "      <td>Grifel \"Tea Time\"</td>\n",
       "      <td>clear author two goal mind 1 take advantag ame...</td>\n",
       "      <td>1</td>\n",
       "      <td>drivel</td>\n",
       "      <td>06 21, 2003</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3JD07VHDLT5FF</td>\n",
       "      <td>isala \"Isabel and Lars\"</td>\n",
       "      <td>collect stori memori japanes soldier fought bu...</td>\n",
       "      <td>5</td>\n",
       "      <td>compel stori ordinari peopl</td>\n",
       "      <td>03 19, 2005</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1JBVAMC9YU0WD</td>\n",
       "      <td>ED</td>\n",
       "      <td>glad borrow book librari instead part hard ear...</td>\n",
       "      <td>1</td>\n",
       "      <td>chees stink</td>\n",
       "      <td>09 25, 2000</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1O7OXC13G9X3E</td>\n",
       "      <td>kayp75 \"kayp75\"</td>\n",
       "      <td>realli enjoy put charact describ beauti felt c...</td>\n",
       "      <td>5</td>\n",
       "      <td>great read</td>\n",
       "      <td>10 21, 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A1RAUVCWYHTQI4</td>\n",
       "      <td>A. Ross</td>\n",
       "      <td>read lot scienc fiction mayb five novel year t...</td>\n",
       "      <td>3</td>\n",
       "      <td>lot good element fail add engag read</td>\n",
       "      <td>08 4, 2010</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A3PZTH1DTX6O6B</td>\n",
       "      <td>H. F. Gibbard \"History Buff\"</td>\n",
       "      <td>legendari pornograph larri flynt team historia...</td>\n",
       "      <td>4</td>\n",
       "      <td>fun occasion debat histori presidenti sex</td>\n",
       "      <td>06 11, 2011</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                             reviewerName  \\\n",
       "0  A3UPFTGAWZ3G2R                          David J. Loftus   \n",
       "1  A1XTKTLNSCRLDS                          Ellen Rappaport   \n",
       "2  A1A77B6DQQH436                           crescamp \"esc\"   \n",
       "3    AEAF4MRYHJZI  Angelia Menchan \"acvermen.blogspot.com\"   \n",
       "4  A3B7KU72LGWFER                        Grifel \"Tea Time\"   \n",
       "5  A3JD07VHDLT5FF                  isala \"Isabel and Lars\"   \n",
       "6  A1JBVAMC9YU0WD                                       ED   \n",
       "7  A1O7OXC13G9X3E                          kayp75 \"kayp75\"   \n",
       "8  A1RAUVCWYHTQI4                                  A. Ross   \n",
       "9  A3PZTH1DTX6O6B             H. F. Gibbard \"History Buff\"   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  jenkin histori professor member parliament wel...        4   \n",
       "1  detect inspector erlendur sveinsson best uncov...        5   \n",
       "2  read purchas gift famili small children hope m...        3   \n",
       "3  fierc angel sheri park read like dissert afric...        4   \n",
       "4  clear author two goal mind 1 take advantag ame...        1   \n",
       "5  collect stori memori japanes soldier fought bu...        5   \n",
       "6  glad borrow book librari instead part hard ear...        1   \n",
       "7  realli enjoy put charact describ beauti felt c...        5   \n",
       "8  read lot scienc fiction mayb five novel year t...        3   \n",
       "9  legendari pornograph larri flynt team historia...        4   \n",
       "\n",
       "                                     summary   reviewTime  rates_count  \\\n",
       "0                     quit readabl nice done   12 6, 2001           40   \n",
       "1                               mesmer depth  02 23, 2014            0   \n",
       "2                   10 minut life lesson kid  02 12, 2013            3   \n",
       "3                                      fierc  03 24, 2010            9   \n",
       "4                                     drivel  06 21, 2003           19   \n",
       "5                compel stori ordinari peopl  03 19, 2005            7   \n",
       "6                                chees stink  09 25, 2000            6   \n",
       "7                                 great read  10 21, 2013            0   \n",
       "8       lot good element fail add engag read   08 4, 2010            3   \n",
       "9  fun occasion debat histori presidenti sex  06 11, 2011            5   \n",
       "\n",
       "   helpful_count  rating  \n",
       "0             37       4  \n",
       "1              0       5  \n",
       "2              0       3  \n",
       "3              9       4  \n",
       "4             13       1  \n",
       "5              5       5  \n",
       "6              5       1  \n",
       "7              0       5  \n",
       "8              2       3  \n",
       "9              4       4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(data_set) \n",
    "df['reviewText'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"reviewText\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df)\n",
    "df['summary'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"summary\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df)\n",
    "\n",
    "# Load the stuff to generalize\n",
    "df_to_generalize = pd.read_csv(data_set_to_generalize) \n",
    "df_to_generalize['reviewText'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"reviewText\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df_to_generalize)\n",
    "df_to_generalize['summary'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"summary\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df_to_generalize)\n",
    "\n",
    "\n",
    "\n",
    "# Show basic info\n",
    "print(df.dtypes)            # all columns/types\n",
    "print(\"Shape: \",df.shape)   # shape\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries:  49992\n",
      "\n",
      "Distribution of the column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5    39420\n",
       "1     5862\n",
       "0     4710\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nan Values:  0\n"
     ]
    }
   ],
   "source": [
    "# Add label to df (without this it's not possible to train the model)\n",
    "df[label] = TransformLabels(threshold=.75).fit_transform(df)  \n",
    "# Show label distribution\n",
    "pc.showColumnDistribution(df,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries:  10572\n",
      "\n",
      "Distribution of the column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    5862\n",
       "0    4710\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nan Values:  0\n"
     ]
    }
   ],
   "source": [
    "# Drop bad stuff?\n",
    "df = df[df[label] != 5]\n",
    "pc.showColumnDistribution(df,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>rates_count</th>\n",
       "      <th>helpful_count</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3UPFTGAWZ3G2R</td>\n",
       "      <td>David J. Loftus</td>\n",
       "      <td>jenkin histori professor member parliament wel...</td>\n",
       "      <td>4</td>\n",
       "      <td>quit readabl nice done</td>\n",
       "      <td>12 6, 2001</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEAF4MRYHJZI</td>\n",
       "      <td>Angelia Menchan \"acvermen.blogspot.com\"</td>\n",
       "      <td>fierc angel sheri park read like dissert afric...</td>\n",
       "      <td>4</td>\n",
       "      <td>fierc</td>\n",
       "      <td>03 24, 2010</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3B7KU72LGWFER</td>\n",
       "      <td>Grifel \"Tea Time\"</td>\n",
       "      <td>clear author two goal mind 1 take advantag ame...</td>\n",
       "      <td>1</td>\n",
       "      <td>drivel</td>\n",
       "      <td>06 21, 2003</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3JD07VHDLT5FF</td>\n",
       "      <td>isala \"Isabel and Lars\"</td>\n",
       "      <td>collect stori memori japanes soldier fought bu...</td>\n",
       "      <td>5</td>\n",
       "      <td>compel stori ordinari peopl</td>\n",
       "      <td>03 19, 2005</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1JBVAMC9YU0WD</td>\n",
       "      <td>ED</td>\n",
       "      <td>glad borrow book librari instead part hard ear...</td>\n",
       "      <td>1</td>\n",
       "      <td>chees stink</td>\n",
       "      <td>09 25, 2000</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                             reviewerName  \\\n",
       "0  A3UPFTGAWZ3G2R                          David J. Loftus   \n",
       "3    AEAF4MRYHJZI  Angelia Menchan \"acvermen.blogspot.com\"   \n",
       "4  A3B7KU72LGWFER                        Grifel \"Tea Time\"   \n",
       "5  A3JD07VHDLT5FF                  isala \"Isabel and Lars\"   \n",
       "6  A1JBVAMC9YU0WD                                       ED   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  jenkin histori professor member parliament wel...        4   \n",
       "3  fierc angel sheri park read like dissert afric...        4   \n",
       "4  clear author two goal mind 1 take advantag ame...        1   \n",
       "5  collect stori memori japanes soldier fought bu...        5   \n",
       "6  glad borrow book librari instead part hard ear...        1   \n",
       "\n",
       "                       summary   reviewTime  rates_count  helpful_count  \\\n",
       "0       quit readabl nice done   12 6, 2001           40             37   \n",
       "3                        fierc  03 24, 2010            9              9   \n",
       "4                       drivel  06 21, 2003           19             13   \n",
       "5  compel stori ordinari peopl  03 19, 2005            7              5   \n",
       "6                  chees stink  09 25, 2000            6              5   \n",
       "\n",
       "   rating  label  \n",
       "0       4      1  \n",
       "3       4      1  \n",
       "4       1      0  \n",
       "5       5      0  \n",
       "6       1      1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "docs_reviewText = df['reviewText'].tolist()\n",
    "docs_summary = df['summary'].tolist()\n",
    "\n",
    "# df to generalize \n",
    "docs_reviewText_generalize = df_to_generalize['reviewText'].tolist()\n",
    "docs_summary_generalize = df_to_generalize['summary'].tolist()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10572/10572 [01:27<00:00, 120.74it/s]\n",
      "100%|██████████| 10572/10572 [00:03<00:00, 2659.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# df\n",
    "nlp_docs__docs_reviewText = list(tqdm(nlp.pipe(docs_reviewText), total=len(docs_reviewText)))\n",
    "nlp_docs__docs_summary = list(tqdm(nlp.pipe(docs_summary), total=len(docs_summary)))\n",
    "\n",
    "# df to generalize \n",
    "nlp_docs__docs_reviewText_generalize = list(tqdm(nlp.pipe(docs_reviewText_generalize), total=len(docs_reviewText_generalize)))\n",
    "nlp_docs__docs_summary_generalize = list(tqdm(nlp.pipe(docs_summary_generalize), total=len(docs_summary_generalize)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedrocondeco/.local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3420: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/pedrocondeco/.local/lib/python3.7/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/pedrocondeco/.local/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>rating</th>\n",
       "      <th>summarynb_words</th>\n",
       "      <th>summaryav_word_size</th>\n",
       "      <th>summarynb_unique_words</th>\n",
       "      <th>summarynb_PoS</th>\n",
       "      <th>reviewTextnb_words</th>\n",
       "      <th>reviewTextav_word_size</th>\n",
       "      <th>reviewTextnb_unique_words</th>\n",
       "      <th>reviewTextnb_PoS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2HESNQJZ9OB7H</td>\n",
       "      <td>Jen</td>\n",
       "      <td>bore stupid hard time finish plot close discri...</td>\n",
       "      <td>unbeliev</td>\n",
       "      <td>02 16, 2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>4.509434</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1ABXPSFA9PC8N</td>\n",
       "      <td>Ben Parker</td>\n",
       "      <td>ill first admit best cook download book want s...</td>\n",
       "      <td>easi clear cook</td>\n",
       "      <td>11 7, 2012</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>4.925000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AYVW3O6W8S5S4</td>\n",
       "      <td>Johnny in Texas</td>\n",
       "      <td>tell anyth spotlight home peopl cobbl togeth l...</td>\n",
       "      <td>bad</td>\n",
       "      <td>02 25, 2014</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A17GOTFSUAWN17</td>\n",
       "      <td>Tyson</td>\n",
       "      <td>whenev met last two girlfriend person script s...</td>\n",
       "      <td>short book good primer text girl</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "      <td>4.983471</td>\n",
       "      <td>0.231405</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2VO8K861AV83R</td>\n",
       "      <td>Avatheps \"Avatheps\"</td>\n",
       "      <td>read review decid take chanc collect seem offe...</td>\n",
       "      <td>disappoint could finish</td>\n",
       "      <td>12 30, 2013</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>5.076923</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>A3SIU6DPNBMRIS</td>\n",
       "      <td>Torben Capiau</td>\n",
       "      <td>book written well stori easi understand enjoy ...</td>\n",
       "      <td>sweet book</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>5.062500</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>A201Y92H6J38KS</td>\n",
       "      <td>Mariekris D. Silva \"little bookworm\"</td>\n",
       "      <td>expect complet stori sore disappont stori nice...</td>\n",
       "      <td>happen</td>\n",
       "      <td>05 6, 2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5.437500</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>A1Y9H69VZQY6BY</td>\n",
       "      <td>Matthew S Blais</td>\n",
       "      <td>littl predict good light read charactersar wel...</td>\n",
       "      <td>good stori</td>\n",
       "      <td>04 18, 2013</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>A1IK5W2W4AFRZ3</td>\n",
       "      <td>The BookWorm \"Reader\"</td>\n",
       "      <td>book take place expect go great write great ch...</td>\n",
       "      <td>wow great book</td>\n",
       "      <td>03 28, 2013</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>5.051724</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>A2ZJ2KR4ADH0E</td>\n",
       "      <td>Kindle Customer</td>\n",
       "      <td>find book great resourc learn guitar fretboard...</td>\n",
       "      <td>fretboard</td>\n",
       "      <td>09 13, 2013</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5.470588</td>\n",
       "      <td>1.294118</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          reviewerID                          reviewerName  \\\n",
       "0     A2HESNQJZ9OB7H                                   Jen   \n",
       "1     A1ABXPSFA9PC8N                            Ben Parker   \n",
       "2      AYVW3O6W8S5S4                       Johnny in Texas   \n",
       "3     A17GOTFSUAWN17                                 Tyson   \n",
       "4     A2VO8K861AV83R                   Avatheps \"Avatheps\"   \n",
       "...              ...                                   ...   \n",
       "1995  A3SIU6DPNBMRIS                         Torben Capiau   \n",
       "1996  A201Y92H6J38KS  Mariekris D. Silva \"little bookworm\"   \n",
       "1997  A1Y9H69VZQY6BY                       Matthew S Blais   \n",
       "1998  A1IK5W2W4AFRZ3                 The BookWorm \"Reader\"   \n",
       "1999   A2ZJ2KR4ADH0E                       Kindle Customer   \n",
       "\n",
       "                                             reviewText  \\\n",
       "0     bore stupid hard time finish plot close discri...   \n",
       "1     ill first admit best cook download book want s...   \n",
       "2     tell anyth spotlight home peopl cobbl togeth l...   \n",
       "3     whenev met last two girlfriend person script s...   \n",
       "4     read review decid take chanc collect seem offe...   \n",
       "...                                                 ...   \n",
       "1995  book written well stori easi understand enjoy ...   \n",
       "1996  expect complet stori sore disappont stori nice...   \n",
       "1997  littl predict good light read charactersar wel...   \n",
       "1998  book take place expect go great write great ch...   \n",
       "1999  find book great resourc learn guitar fretboard...   \n",
       "\n",
       "                               summary   reviewTime  rating  summarynb_words  \\\n",
       "0                             unbeliev  02 16, 2014       1                1   \n",
       "1                      easi clear cook   11 7, 2012       5                3   \n",
       "2                                  bad  02 25, 2014       3                1   \n",
       "3     short book good primer text girl  03 15, 2014       4                6   \n",
       "4              disappoint could finish  12 30, 2013       2                3   \n",
       "...                                ...          ...     ...              ...   \n",
       "1995                        sweet book  02 19, 2014       5                2   \n",
       "1996                            happen   05 6, 2013       3                1   \n",
       "1997                        good stori  04 18, 2013       4                2   \n",
       "1998                    wow great book  03 28, 2013       5                3   \n",
       "1999                         fretboard  09 13, 2013       4                1   \n",
       "\n",
       "      summaryav_word_size  summarynb_unique_words  summarynb_PoS  \\\n",
       "0                8.000000                7.000000              1   \n",
       "1                4.333333                3.333333              3   \n",
       "2                3.000000                3.000000              1   \n",
       "3                4.500000                2.666667              6   \n",
       "4                7.000000                4.666667              2   \n",
       "...                   ...                     ...            ...   \n",
       "1995             4.500000                4.000000              2   \n",
       "1996             6.000000                5.000000              1   \n",
       "1997             4.500000                4.000000              2   \n",
       "1998             4.000000                3.333333              2   \n",
       "1999             9.000000                8.000000              0   \n",
       "\n",
       "      reviewTextnb_words  reviewTextav_word_size  reviewTextnb_unique_words  \\\n",
       "0                     53                4.509434                   0.433962   \n",
       "1                     40                4.925000                   0.600000   \n",
       "2                     15                5.000000                   1.466667   \n",
       "3                    121                4.983471                   0.231405   \n",
       "4                     91                5.076923                   0.307692   \n",
       "...                  ...                     ...                        ...   \n",
       "1995                  32                5.062500                   0.718750   \n",
       "1996                  16                5.437500                   1.187500   \n",
       "1997                  14                5.500000                   1.357143   \n",
       "1998                  58                5.051724                   0.396552   \n",
       "1999                  17                5.470588                   1.294118   \n",
       "\n",
       "      reviewTextnb_PoS  \n",
       "0                   47  \n",
       "1                   31  \n",
       "2                    9  \n",
       "3                   91  \n",
       "4                   66  \n",
       "...                ...  \n",
       "1995                25  \n",
       "1996                12  \n",
       "1997                12  \n",
       "1998                46  \n",
       "1999                14  \n",
       "\n",
       "[2000 rows x 14 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df\n",
    "add_extra_features(df, \"summary\", docs_summary, nlp_docs__docs_summary)\n",
    "add_extra_features(df, \"reviewText\", docs_reviewText, nlp_docs__docs_reviewText)\n",
    "\n",
    "# df to generalize \n",
    "add_extra_features(df_to_generalize, \"summary\", docs_summary_generalize, nlp_docs__docs_summary_generalize)\n",
    "add_extra_features(df_to_generalize, \"reviewText\", docs_reviewText_generalize, nlp_docs__docs_reviewText_generalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0, inplace=True)\n",
    "df_to_generalize.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>rating</th>\n",
       "      <th>summarynb_words</th>\n",
       "      <th>summaryav_word_size</th>\n",
       "      <th>summarynb_unique_words</th>\n",
       "      <th>summarynb_PoS</th>\n",
       "      <th>reviewTextnb_words</th>\n",
       "      <th>reviewTextav_word_size</th>\n",
       "      <th>reviewTextnb_unique_words</th>\n",
       "      <th>reviewTextnb_PoS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A2HESNQJZ9OB7H</td>\n",
       "      <td>Jen</td>\n",
       "      <td>bore stupid hard time finish plot close discri...</td>\n",
       "      <td>unbeliev</td>\n",
       "      <td>02 16, 2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>4.509434</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1ABXPSFA9PC8N</td>\n",
       "      <td>Ben Parker</td>\n",
       "      <td>ill first admit best cook download book want s...</td>\n",
       "      <td>easi clear cook</td>\n",
       "      <td>11 7, 2012</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>4.925000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AYVW3O6W8S5S4</td>\n",
       "      <td>Johnny in Texas</td>\n",
       "      <td>tell anyth spotlight home peopl cobbl togeth l...</td>\n",
       "      <td>bad</td>\n",
       "      <td>02 25, 2014</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A17GOTFSUAWN17</td>\n",
       "      <td>Tyson</td>\n",
       "      <td>whenev met last two girlfriend person script s...</td>\n",
       "      <td>short book good primer text girl</td>\n",
       "      <td>03 15, 2014</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "      <td>4.983471</td>\n",
       "      <td>0.231405</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2VO8K861AV83R</td>\n",
       "      <td>Avatheps \"Avatheps\"</td>\n",
       "      <td>read review decid take chanc collect seem offe...</td>\n",
       "      <td>disappoint could finish</td>\n",
       "      <td>12 30, 2013</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>91</td>\n",
       "      <td>5.076923</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>A3SIU6DPNBMRIS</td>\n",
       "      <td>Torben Capiau</td>\n",
       "      <td>book written well stori easi understand enjoy ...</td>\n",
       "      <td>sweet book</td>\n",
       "      <td>02 19, 2014</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>5.062500</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>A201Y92H6J38KS</td>\n",
       "      <td>Mariekris D. Silva \"little bookworm\"</td>\n",
       "      <td>expect complet stori sore disappont stori nice...</td>\n",
       "      <td>happen</td>\n",
       "      <td>05 6, 2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5.437500</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>A1Y9H69VZQY6BY</td>\n",
       "      <td>Matthew S Blais</td>\n",
       "      <td>littl predict good light read charactersar wel...</td>\n",
       "      <td>good stori</td>\n",
       "      <td>04 18, 2013</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>1.357143</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>A1IK5W2W4AFRZ3</td>\n",
       "      <td>The BookWorm \"Reader\"</td>\n",
       "      <td>book take place expect go great write great ch...</td>\n",
       "      <td>wow great book</td>\n",
       "      <td>03 28, 2013</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>5.051724</td>\n",
       "      <td>0.396552</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>A2ZJ2KR4ADH0E</td>\n",
       "      <td>Kindle Customer</td>\n",
       "      <td>find book great resourc learn guitar fretboard...</td>\n",
       "      <td>fretboard</td>\n",
       "      <td>09 13, 2013</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5.470588</td>\n",
       "      <td>1.294118</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          reviewerID                          reviewerName  \\\n",
       "0     A2HESNQJZ9OB7H                                   Jen   \n",
       "1     A1ABXPSFA9PC8N                            Ben Parker   \n",
       "2      AYVW3O6W8S5S4                       Johnny in Texas   \n",
       "3     A17GOTFSUAWN17                                 Tyson   \n",
       "4     A2VO8K861AV83R                   Avatheps \"Avatheps\"   \n",
       "...              ...                                   ...   \n",
       "1995  A3SIU6DPNBMRIS                         Torben Capiau   \n",
       "1996  A201Y92H6J38KS  Mariekris D. Silva \"little bookworm\"   \n",
       "1997  A1Y9H69VZQY6BY                       Matthew S Blais   \n",
       "1998  A1IK5W2W4AFRZ3                 The BookWorm \"Reader\"   \n",
       "1999   A2ZJ2KR4ADH0E                       Kindle Customer   \n",
       "\n",
       "                                             reviewText  \\\n",
       "0     bore stupid hard time finish plot close discri...   \n",
       "1     ill first admit best cook download book want s...   \n",
       "2     tell anyth spotlight home peopl cobbl togeth l...   \n",
       "3     whenev met last two girlfriend person script s...   \n",
       "4     read review decid take chanc collect seem offe...   \n",
       "...                                                 ...   \n",
       "1995  book written well stori easi understand enjoy ...   \n",
       "1996  expect complet stori sore disappont stori nice...   \n",
       "1997  littl predict good light read charactersar wel...   \n",
       "1998  book take place expect go great write great ch...   \n",
       "1999  find book great resourc learn guitar fretboard...   \n",
       "\n",
       "                               summary   reviewTime  rating  summarynb_words  \\\n",
       "0                             unbeliev  02 16, 2014       1                1   \n",
       "1                      easi clear cook   11 7, 2012       5                3   \n",
       "2                                  bad  02 25, 2014       3                1   \n",
       "3     short book good primer text girl  03 15, 2014       4                6   \n",
       "4              disappoint could finish  12 30, 2013       2                3   \n",
       "...                                ...          ...     ...              ...   \n",
       "1995                        sweet book  02 19, 2014       5                2   \n",
       "1996                            happen   05 6, 2013       3                1   \n",
       "1997                        good stori  04 18, 2013       4                2   \n",
       "1998                    wow great book  03 28, 2013       5                3   \n",
       "1999                         fretboard  09 13, 2013       4                1   \n",
       "\n",
       "      summaryav_word_size  summarynb_unique_words  summarynb_PoS  \\\n",
       "0                8.000000                7.000000              1   \n",
       "1                4.333333                3.333333              3   \n",
       "2                3.000000                3.000000              1   \n",
       "3                4.500000                2.666667              6   \n",
       "4                7.000000                4.666667              2   \n",
       "...                   ...                     ...            ...   \n",
       "1995             4.500000                4.000000              2   \n",
       "1996             6.000000                5.000000              1   \n",
       "1997             4.500000                4.000000              2   \n",
       "1998             4.000000                3.333333              2   \n",
       "1999             9.000000                8.000000              0   \n",
       "\n",
       "      reviewTextnb_words  reviewTextav_word_size  reviewTextnb_unique_words  \\\n",
       "0                     53                4.509434                   0.433962   \n",
       "1                     40                4.925000                   0.600000   \n",
       "2                     15                5.000000                   1.466667   \n",
       "3                    121                4.983471                   0.231405   \n",
       "4                     91                5.076923                   0.307692   \n",
       "...                  ...                     ...                        ...   \n",
       "1995                  32                5.062500                   0.718750   \n",
       "1996                  16                5.437500                   1.187500   \n",
       "1997                  14                5.500000                   1.357143   \n",
       "1998                  58                5.051724                   0.396552   \n",
       "1999                  17                5.470588                   1.294118   \n",
       "\n",
       "      reviewTextnb_PoS  \n",
       "0                   47  \n",
       "1                   31  \n",
       "2                    9  \n",
       "3                   91  \n",
       "4                   66  \n",
       "...                ...  \n",
       "1995                25  \n",
       "1996                12  \n",
       "1997                12  \n",
       "1998                46  \n",
       "1999                14  \n",
       "\n",
       "[2000 rows x 14 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedrocondeco/.local/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_label = df_train['label']\n",
    "df_train.drop(label, inplace=True, axis=1)   # labels MUST NOT exist within train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features here, keep standard, concatenate _feature in the end\n",
    "review_text_feature = Pipeline([\n",
    "                                ('review_text_selector', TextSelector(\"reviewText\")),\n",
    "                                ('tfidf', TfidfVectorizer())\n",
    "                            ])\n",
    "summary_feature = Pipeline([\n",
    "                                ('summary_selector', TextSelector(\"summary\")),\n",
    "                                ('tfidf', TfidfVectorizer())\n",
    "                            ])\n",
    "rating_feature = Pipeline([\n",
    "                                ('rating_selector', NumberSelector(\"rating\")),\n",
    "                                ('onehot', OneHotEncoder())\n",
    "                            ])\n",
    "summarynb_words = Pipeline([\n",
    "                                ('summarynb_words', NumberSelector(\"summarynb_words\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])\n",
    "summaryav_word_size = Pipeline([\n",
    "                                ('summaryav_word_size', NumberSelector(\"summaryav_word_size\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])                           \n",
    "summarynb_unique_words = Pipeline([\n",
    "                                ('summarynb_unique_words', NumberSelector(\"summarynb_unique_words\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])                           \n",
    "summarynb_PoS = Pipeline([\n",
    "                                ('summarynb_PoS', NumberSelector(\"summarynb_PoS\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "reviewTextnb_words = Pipeline([\n",
    "                                ('reviewTextnb_words', NumberSelector(\"reviewTextnb_words\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])\n",
    "reviewTextav_word_size = Pipeline([\n",
    "                                ('reviewTextav_word_size', NumberSelector(\"reviewTextav_word_size\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])                           \n",
    "reviewTextnb_unique_words = Pipeline([\n",
    "                                ('reviewTextnb_unique_words', NumberSelector(\"reviewTextnb_unique_words\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])                           \n",
    "reviewTextnb_PoS = Pipeline([\n",
    "                                ('reviewTextnb_PoS', NumberSelector(\"reviewTextnb_PoS\")),\n",
    "                                ('standardScaler', StandardScaler())\n",
    "                            ])  \n",
    "\n",
    "\n",
    "# Features\n",
    "features = FeatureUnion([\n",
    "                        ('summary_feature',summary_feature),\n",
    "                        ('review_text_feature',review_text_feature),\n",
    "                        ('rating_feature', rating_feature),\n",
    "                        ('summarynb_words',summarynb_words),\n",
    "                        ('summaryav_word_size',summaryav_word_size),\n",
    "                        ('summarynb_unique_words',summarynb_unique_words),\n",
    "                        ('summarynb_PoS',summarynb_PoS),\n",
    "                        ('reviewTextnb_words',reviewTextnb_words),\n",
    "                        ('reviewTextav_word_size',reviewTextav_word_size),\n",
    "                        ('reviewTextnb_unique_words',reviewTextnb_unique_words),\n",
    "                        ('reviewTextnb_PoS',reviewTextnb_PoS),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10043, 48495)\n"
     ]
    }
   ],
   "source": [
    "ok = features.fit_transform(df_train)\n",
    "print(ok.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline w/ model\n",
    "model_pipeline = Pipeline([\n",
    "                        ('features', features),\n",
    "                        ('model',RandomForestClassifier(random_state = 42))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline and print silly values\n",
    "# returns model\n",
    "def baseline(train_set, train_labels, test_set, model):\n",
    "  # Train the classifier\n",
    "  model.fit(train_set, train_labels)\n",
    "\n",
    "  # Predict\n",
    "  model_predictions = model.predict(test_set)\n",
    "\n",
    "  # Print F1 stuff\n",
    "  print(classification_report(test_set[label], model_predictions))\n",
    "\n",
    "  _acc = np.mean(model_predictions == test_set[label])\n",
    "  print('Accuracy: ' + str(_acc))\n",
    "\n",
    "  return model\n",
    "\n",
    "def stupidFunction(x):\n",
    "    if x == 0:\n",
    "        return \"false\"\n",
    "    return \"true\"\n",
    "def predictFromFileAndOutPutCSV(model, df_to_generalize, path):\n",
    "    predictions = model.predict(df_to_generalize)\n",
    "    out = pd.DataFrame({'is_helpful': predictions})\n",
    "    out['is_helpful'] =  out['is_helpful'].apply(lambda x: stupidFunction(x))\n",
    "    out.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.57      0.53       933\n",
      "           1       0.61      0.53      0.57      1182\n",
      "\n",
      "    accuracy                           0.55      2115\n",
      "   macro avg       0.55      0.55      0.55      2115\n",
      "weighted avg       0.56      0.55      0.55      2115\n",
      "\n",
      "Accuracy: 0.548936170212766\n"
     ]
    }
   ],
   "source": [
    "model = baseline(df_train, train_label, df_test, model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictFromFileAndOutPutCSV(model, df_to_generalize,'./quinta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}