{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import executable, argv\n",
    "from subprocess import check_output\n",
    "import pctils as pc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sys import executable, argv\n",
    "from subprocess import check_output\n",
    "import pctils as pc\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import spacy\n",
    "import hashlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pedrocondeco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import TransformerMixin\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transformers\n",
    "def apply_tokenizer(data, tokenizer):\n",
    "    lista_tok = [\" \".join(tokenizer.tokenize(x)) for x in data]\n",
    "    return lista_tok\n",
    "\n",
    "def apply_lowercase(data):\n",
    "    list_lower = [x.lower() for x in data]\n",
    "    return list_lower\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "def apply_filter_stopwords(data, stopword_list):\n",
    "    data_no_stopwords = []\n",
    "    for x in data:\n",
    "        aux = \" \".join([w for w in x.split() if w not in stopword_list])\n",
    "        data_no_stopwords.append(aux)\n",
    "    return data_no_stopwords\n",
    "\n",
    "def apply_filter_punct(data):\n",
    "    data_no_punct = []\n",
    "    for x in data:\n",
    "        \n",
    "        aux = \"\".join([w for w in x if w not in string.punctuation])\n",
    "        data_no_punct.append(aux)\n",
    "\n",
    "    return data_no_punct\n",
    "\n",
    "def normalize_whitespace(data):\n",
    "    return [re.sub(r\"^\\s+|\\s+$|(?<=\\s)\\s*\", \"\", text) for text in data]\n",
    "\n",
    "def apply_stemmer(data, stemmer):\n",
    "    list_tok = [WordPunctTokenizer().tokenize(x) for x in data]\n",
    "    stems = [\" \".join(list(map(stemmer.stem, y))) for y in list_tok]\n",
    "    return stems\n",
    "\n",
    "# creates a new column helpful_count/rates_count\n",
    "class TransformLabels(TransformerMixin):\n",
    "    def __init__(self, threshold = .5):\n",
    "        self.helpful_count = 'helpful_count'\n",
    "        self.rates_count = 'rates_count'\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def isHelpful(self, helpful_count, rates_count):\n",
    "        if rates_count == 0 or rates_count < 5:\n",
    "            return 5 # we don't know, not helpfull for now\n",
    "        if helpful_count > rates_count:\n",
    "            return 5 # five means bad, needs to be droped\n",
    "        val = helpful_count/rates_count\n",
    "        if val >= self.threshold:\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "\n",
    "        label_array = []\n",
    "        for h, r in zip(_X[self.helpful_count], _X[self.rates_count]):\n",
    "            is_helpul = self.isHelpful(h,r)\n",
    "            label_array += [is_helpul]\n",
    "\n",
    "        out = pd.Series(label_array)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class TextTransformer(TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        words = _X[self.key]\n",
    "        features =  TfidfTransformer().fit_transform(words)\n",
    "        return features\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class LazyTranformer(TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        return _X[self.key].to_frame()\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class Selector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "\n",
    "class TextSelector(Selector):\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "\n",
    "class NumberSelector(Selector):\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, column_key, tokenizer, lower=True, remove_punct=True, stopwords=[], stemmer=None):\n",
    "        self.column_key = column_key\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, data):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        sentences_preprocessed = apply_tokenizer(data, self.tokenizer)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "            sentences_preprocessed = apply_lowercase(sentences_preprocessed)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        if self.stopwords:\n",
    "            sentences_preprocessed = apply_filter_stopwords(sentences_preprocessed,self.stopwords)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            sentences_preprocessed = apply_filter_punct(sentences_preprocessed)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        sentences_preprocessed = normalize_whitespace(sentences_preprocessed)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "    \n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            sentences_preprocessed = apply_stemmer(sentences_preprocessed,self.stemmer)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        return sentences_preprocessed\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        _X = X.copy()\n",
    "        column_to_transform = _X[self.column_key]\n",
    "        return self.clean_sentences(column_to_transform)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "#data_set = pc.getFile()  # gets path of tfile, you may replace with a string\n",
    "data_set=\"/home/pedrocondeco/ldsa/group3/data/book_review_labelled_data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "#data_set_to_generalize = pc.getFile()  # gets path of tfile, you may replace with a string\n",
    "data_set_to_generalize=\"/home/pedrocondeco/ldsa/group3/data/book_review_test_data_unlabelled.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "label = \"label\" # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewerID       object\n",
      "reviewerName     object\n",
      "reviewText       object\n",
      "overall           int64\n",
      "summary          object\n",
      "reviewTime       object\n",
      "rates_count       int64\n",
      "helpful_count     int64\n",
      "rating            int64\n",
      "dtype: object\n",
      "Shape:  (49992, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>rates_count</th>\n",
       "      <th>helpful_count</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3UPFTGAWZ3G2R</td>\n",
       "      <td>David J. Loftus</td>\n",
       "      <td>jenkin histori professor member parliament wel...</td>\n",
       "      <td>4</td>\n",
       "      <td>quit readabl nice done</td>\n",
       "      <td>12 6, 2001</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1XTKTLNSCRLDS</td>\n",
       "      <td>Ellen Rappaport</td>\n",
       "      <td>detect inspector erlendur sveinsson best uncov...</td>\n",
       "      <td>5</td>\n",
       "      <td>mesmer depth</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1A77B6DQQH436</td>\n",
       "      <td>crescamp \"esc\"</td>\n",
       "      <td>read purchas gift famili small children hope m...</td>\n",
       "      <td>3</td>\n",
       "      <td>10 minut life lesson kid</td>\n",
       "      <td>02 12, 2013</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEAF4MRYHJZI</td>\n",
       "      <td>Angelia Menchan \"acvermen.blogspot.com\"</td>\n",
       "      <td>fierc angel sheri park read like dissert afric...</td>\n",
       "      <td>4</td>\n",
       "      <td>fierc</td>\n",
       "      <td>03 24, 2010</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A3B7KU72LGWFER</td>\n",
       "      <td>Grifel \"Tea Time\"</td>\n",
       "      <td>clear author two goal mind 1 take advantag ame...</td>\n",
       "      <td>1</td>\n",
       "      <td>drivel</td>\n",
       "      <td>06 21, 2003</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A3JD07VHDLT5FF</td>\n",
       "      <td>isala \"Isabel and Lars\"</td>\n",
       "      <td>collect stori memori japanes soldier fought bu...</td>\n",
       "      <td>5</td>\n",
       "      <td>compel stori ordinari peopl</td>\n",
       "      <td>03 19, 2005</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A1JBVAMC9YU0WD</td>\n",
       "      <td>ED</td>\n",
       "      <td>glad borrow book librari instead part hard ear...</td>\n",
       "      <td>1</td>\n",
       "      <td>chees stink</td>\n",
       "      <td>09 25, 2000</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A1O7OXC13G9X3E</td>\n",
       "      <td>kayp75 \"kayp75\"</td>\n",
       "      <td>realli enjoy put charact describ beauti felt c...</td>\n",
       "      <td>5</td>\n",
       "      <td>great read</td>\n",
       "      <td>10 21, 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A1RAUVCWYHTQI4</td>\n",
       "      <td>A. Ross</td>\n",
       "      <td>read lot scienc fiction mayb five novel year t...</td>\n",
       "      <td>3</td>\n",
       "      <td>lot good element fail add engag read</td>\n",
       "      <td>08 4, 2010</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A3PZTH1DTX6O6B</td>\n",
       "      <td>H. F. Gibbard \"History Buff\"</td>\n",
       "      <td>legendari pornograph larri flynt team historia...</td>\n",
       "      <td>4</td>\n",
       "      <td>fun occasion debat histori presidenti sex</td>\n",
       "      <td>06 11, 2011</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID                             reviewerName  \\\n",
       "0  A3UPFTGAWZ3G2R                          David J. Loftus   \n",
       "1  A1XTKTLNSCRLDS                          Ellen Rappaport   \n",
       "2  A1A77B6DQQH436                           crescamp \"esc\"   \n",
       "3    AEAF4MRYHJZI  Angelia Menchan \"acvermen.blogspot.com\"   \n",
       "4  A3B7KU72LGWFER                        Grifel \"Tea Time\"   \n",
       "5  A3JD07VHDLT5FF                  isala \"Isabel and Lars\"   \n",
       "6  A1JBVAMC9YU0WD                                       ED   \n",
       "7  A1O7OXC13G9X3E                          kayp75 \"kayp75\"   \n",
       "8  A1RAUVCWYHTQI4                                  A. Ross   \n",
       "9  A3PZTH1DTX6O6B             H. F. Gibbard \"History Buff\"   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  jenkin histori professor member parliament wel...        4   \n",
       "1  detect inspector erlendur sveinsson best uncov...        5   \n",
       "2  read purchas gift famili small children hope m...        3   \n",
       "3  fierc angel sheri park read like dissert afric...        4   \n",
       "4  clear author two goal mind 1 take advantag ame...        1   \n",
       "5  collect stori memori japanes soldier fought bu...        5   \n",
       "6  glad borrow book librari instead part hard ear...        1   \n",
       "7  realli enjoy put charact describ beauti felt c...        5   \n",
       "8  read lot scienc fiction mayb five novel year t...        3   \n",
       "9  legendari pornograph larri flynt team historia...        4   \n",
       "\n",
       "                                     summary   reviewTime  rates_count  \\\n",
       "0                     quit readabl nice done   12 6, 2001           40   \n",
       "1                               mesmer depth  02 23, 2014            0   \n",
       "2                   10 minut life lesson kid  02 12, 2013            3   \n",
       "3                                      fierc  03 24, 2010            9   \n",
       "4                                     drivel  06 21, 2003           19   \n",
       "5                compel stori ordinari peopl  03 19, 2005            7   \n",
       "6                                chees stink  09 25, 2000            6   \n",
       "7                                 great read  10 21, 2013            0   \n",
       "8       lot good element fail add engag read   08 4, 2010            3   \n",
       "9  fun occasion debat histori presidenti sex  06 11, 2011            5   \n",
       "\n",
       "   helpful_count  rating  \n",
       "0             37       4  \n",
       "1              0       5  \n",
       "2              0       3  \n",
       "3              9       4  \n",
       "4             13       1  \n",
       "5              5       5  \n",
       "6              5       1  \n",
       "7              0       5  \n",
       "8              2       3  \n",
       "9              4       4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(data_set) \n",
    "df['reviewText'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"reviewText\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df)\n",
    "df['summary'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"summary\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df)\n",
    "\n",
    "# Load the stuff to generalize\n",
    "df_to_generalize = pd.read_csv(data_set_to_generalize) \n",
    "df_to_generalize['reviewText'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"reviewText\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df_to_generalize)\n",
    "df_to_generalize['summary'] = text_cleaner = TextCleanerTransformer(\n",
    "    \"summary\",\n",
    "    WordPunctTokenizer(),\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopwords.words('english'),\n",
    "    stemmer=SnowballStemmer(\"english\"),\n",
    ").transform(df_to_generalize)\n",
    "\n",
    "\n",
    "\n",
    "# Show basic info\n",
    "print(df.dtypes)            # all columns/types\n",
    "print(\"Shape: \",df.shape)   # shape\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries:  49992\n",
      "\n",
      "Distribution of the column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    27871\n",
       "1    22121\n",
       "Name: label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nan Values:  0\n"
     ]
    }
   ],
   "source": [
    "# Add label to df (without this it's not possible to train the model)\n",
    "df[label] = TransformLabels(threshold=.5).fit_transform(df)  \n",
    "# Show label distribution\n",
    "pc.showColumnDistribution(df,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop bad stuff?\n",
    "# df = df[df[label] != 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedrocondeco/.local/lib/python3.7/site-packages/pandas/core/frame.py:4315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "# Split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_label = df_train['label']\n",
    "df_train.drop(label, inplace=True, axis=1)   # labels MUST NOT exist within train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define features here, keep standard, concatenate _feature in the end\n",
    "review_text_feature = Pipeline([\n",
    "                                ('review_text_selector', TextSelector(\"reviewText\")),\n",
    "                                ('tfidf', TfidfVectorizer())\n",
    "                            ])\n",
    "summary_feature = Pipeline([\n",
    "                                ('summary_selector', TextSelector(\"summary\")),\n",
    "                                ('tfidf', TfidfVectorizer())\n",
    "                            ])\n",
    "rating_feature = Pipeline([\n",
    "                                ('rating_selector', NumberSelector(\"rating\")),\n",
    "                                ('onehot', OneHotEncoder())\n",
    "                            ])\n",
    "# Features\n",
    "features = FeatureUnion([\n",
    "                        ('summary_feature',summary_feature),\n",
    "                        ('review_text_feature',review_text_feature),\n",
    "                        ('rating_feature', rating_feature)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39993, 74251)\n"
     ]
    }
   ],
   "source": [
    "ok = features.fit_transform(df_train)\n",
    "print(ok.todense().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline w/ model\n",
    "model_pipeline = Pipeline([\n",
    "                        ('features', features),\n",
    "                        ('model',RandomForestClassifier(random_state = 42))\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline and print silly values\n",
    "# returns model\n",
    "def baseline(train_set, train_labels, test_set, model):\n",
    "  # Train the classifier\n",
    "  model.fit(train_set, train_labels)\n",
    "\n",
    "  # Predict\n",
    "  model_predictions = model.predict(test_set)\n",
    "\n",
    "  # Print F1 stuff\n",
    "  print(classification_report(test_set[label], model_predictions))\n",
    "\n",
    "  _acc = np.mean(model_predictions == test_set[label])\n",
    "  print('Accuracy: ' + str(_acc))\n",
    "\n",
    "  return model\n",
    "\n",
    "def stupidFunction(x):\n",
    "    if x == 0:\n",
    "        return \"false\"\n",
    "    return \"true\"\n",
    "def predictFromFileAndOutPutCSV(model, df_to_generalize, path):\n",
    "    predictions = model.predict(df_to_generalize)\n",
    "    out = pd.DataFrame({'is_helpful': predictions})\n",
    "    out['is_helpful'] =  out['is_helpful'].apply(lambda x: stupidFunction(x))\n",
    "    out.to_csv(path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74      5590\n",
      "           1       0.68      0.55      0.61      4409\n",
      "\n",
      "    accuracy                           0.69      9999\n",
      "   macro avg       0.69      0.67      0.67      9999\n",
      "weighted avg       0.69      0.69      0.68      9999\n",
      "\n",
      "Accuracy: 0.686968696869687\n"
     ]
    }
   ],
   "source": [
    "model = baseline(df_train, train_label, df_test, model_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictFromFileAndOutPutCSV(model, df_to_generalize,'./lol-bom-trabalho.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}